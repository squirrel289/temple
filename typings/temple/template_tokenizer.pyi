"""
This type stub file was generated by pyright.
"""

from typing import Iterator, Literal, Optional, Tuple

"""
temple.template_tokenizer
Core template tokenization engine for Temple DSL.

This is the AUTHORITATIVE implementation used by all Temple components.
Supports configurable delimiters and efficient regex pattern caching.
"""
TokenType = Literal["text", "statement", "expression", "comment"]
class Token:
    def __init__(self, raw_token: str, start: Tuple[int, int], delimiters: Optional[dict[TokenType, tuple[str, str]]] = ...) -> None:
        ...
    
    def __repr__(self): # -> str:
        ...
    


def temple_tokenizer(text: str, delimiters: Optional[dict[TokenType, tuple[str, str]]] = ...) -> Iterator[Token]:
    """
    Yields Token objects for text, statement, expression, and comment regions.
    Supports custom delimiters.

    Regex patterns are cached using functools.lru_cache for performance.
    Subsequent calls with the same delimiter configuration reuse compiled patterns,
    providing 10x+ speedup for batch processing.

    Args:
        text: The template text to tokenize.
        delimiters: Optional dict specifying delimiters for 'statement', 'expression', 'comment'.
            Defaults to Jinja-like delimiters: {%, {{, {#.

    Yields:
        Token objects representing text, statement, expression, or comment regions.
    """
    ...

